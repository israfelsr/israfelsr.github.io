<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blogs on Isra Salazar</title>
    <link>https://israfelsr.github.io/blog/</link>
    <description>Recent content in Blogs on Isra Salazar</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://israfelsr.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Explaining Variational Inference</title>
      <link>https://israfelsr.github.io/blog/variational-inference/</link>
      <pubDate>Mon, 06 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://israfelsr.github.io/blog/variational-inference/</guid>
      <description>How to approximate difficult-to-compute probability densities is an important problem in statistics. Variational Inference (VI) is a statistical inference framework that addresses this problem using optimization. This allows the use of it along with modern and fast optimization techniques which is ideal to approximate probability density functions of large datasets and complex models.
In this post I&amp;rsquo;m going to review Variational Inference, explaining the concepts that it involves, its derivation from the variational methods and its implications in the bayesian inference problem and in current machine learning techniques.</description>
    </item>
    
    <item>
      <title>Understanding Batch, Layer and Group Normalization</title>
      <link>https://israfelsr.github.io/blog/normalizations/</link>
      <pubDate>Wed, 20 May 2020 16:34:50 +0200</pubDate>
      
      <guid>https://israfelsr.github.io/blog/normalizations/</guid>
      <description>This post is an analysis of the actual normalization techniques and why and how to implement them for neural networks. It&amp;rsquo;s based on the paper of Ioffee and Szegedy [1] from 2015, the modification proposed for Layer Normalization [2] and a much more recent work of Group Normalization [3].
Batch Normalization (BN) is a milestone technique in the development of deep learning. - Group Normalization paper.
  One of the big difficulties of deep neural networks is the training stage and its optimization.</description>
    </item>
    
  </channel>
</rss>
